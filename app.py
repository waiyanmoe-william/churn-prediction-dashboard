# Install Streamlit
# No cap, we need that Streamlit rizz frfr ğŸš€âœ¨

import subprocess
import sys

# "Time to listen to phonk and write codes all night ğŸï¸ğŸ’¨"
# This is a version 3 of my Streamlit app with fixed bugs and improved features ğŸ› ï¸ğŸ”¥
# I'm on 4th cup of coffee and 3rd energy drink, let's get it! â˜•ğŸ”¥ğŸ§ 

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

try:
    import streamlit
except ImportError:
    print("Streamlit not found, installing...")
    install("streamlit")

# Import packages
import streamlit as st              # UI so simple even an NPC could use it ğŸ›œğŸ§â€â™‚ï¸
import pandas as pd                 # Wrangling dataframes harder than my sleep schedule ğŸ’¼ğŸ“Š
import numpy as np                  # Math straight outta **backrooms** ğŸ—¿ğŸ”¢
import matplotlib.pyplot as plt     # Visualising the pain (residuals) so we can heal from it ğŸ“‰â¤ï¸â€ğŸ©¹
import seaborn as sns               # Makes basic plots look **aesthetic af** ğŸ¨âœ¨ #TrustTheProcess
import pickle                       # Stashing the model securely like it's off-shore assets ğŸï¸ğŸ’¼
import plotly.express as px         # **3D data but make it âœ¨multiversalâœ¨** ğŸš€ğŸ‘½
import plotly.graph_objects as go   # Nickelback would be proud ğŸ“ˆğŸ¸ **"LOOK AT THIS GRAPH"**
from sklearn.metrics import confusion_matrix, roc_curve, auc        # **Measuring how delulu my model is** ğŸ“ŠğŸ¤¡

# Define feature importance dictionary based on the model findings from Jupyter notebook (version 2)
feature_importance = {
    'Age': 0.3827,                      # Age ate and left no crumbs ğŸ½ï¸ â€“ younger customers ghost more ğŸ‘»
    'NumOfProducts': 0.3136,            # One-product wonders be dipping faster than cryptoscammers ğŸ“¶ğŸ’€
    'IsActiveMember': 0.1125,           # If they ain't active, they ain't loyal ğŸš©
    'Balance': 0.0793,                  # High net worth individuals have diamond hands ğŸ’ğŸ™Œ, it's the liquidity-challenged who fold
    'Geography_Germany': 0.0496,        # German customers be leaving like itâ€™s Oktoberfest ğŸ»âœˆï¸
    'CreditScore': 0.0213,              # Low credit score? Might as well speedrun financial doom ğŸï¸ğŸ”¥
    'EstimatedSalary': 0.0208,          # "Money canâ€™t buy happiness" but it sure buys retention ğŸ’°ğŸ˜Œ
    'Gender': 0.0144,                   # Men slightly more likely to ghost than women ğŸ«¥ğŸ¤·â€â™‚ï¸
    'Tenure': 0.0039,                   # Loyalty kinda mid tbh â€“ doesnâ€™t influence churn much ğŸ¤”
    'Geography_Spain': 0.0009,          # Spanish customers just vibing ğŸ‡ªğŸ‡¸ğŸ’ƒ (not really churning)
    'HasCrCard': 0.0008,                # Credit card holders staying put â€“ free points FTW ğŸŸï¸ğŸ”¥
    'Tenure_Group_Developing': 0.0001,  # Developing customers barely moving the needle ğŸ“‰ğŸ’¤
    'Tenure_Group_Mature': 0.0000       # Mature customers locked in like a gym membership ğŸ‹ï¸â€â™‚ï¸
}

# Page configuration
st.set_page_config(
    page_title="Banking Customer Churn Prediction",
    page_icon="ğŸ’°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# App title and introduction
st.title("Banking Customer Churn Prediction Dashboard")
st.markdown("""
This application demonstrates our AI-based solution for predicting customer churn in the banking industry.
By identifying customers likely to leave, banks can take proactive measures to retain valuable customers.
""")

# Sidebar navigation
st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "Go to",
    ["Overview", "Data Exploration", "Model Performance", "Feature Importance", 
     "Prediction Interface", "Business Insights", "Model Code"]
)

# Load data and model
# Time to load up the goods! ğŸ“¦
# Loading that delulu data ğŸ’…ğŸ§  - main character energy activated
@st.cache_data
def load_data():
    df = pd.read_csv('cleaned_data_with_target.csv')
    feature_df = pd.read_csv('feature_engineered_data.csv')
    return df, feature_df

# Updated prediction function to match new feature importance values (version 2)
def predict_churn(customer_data):
    # Let's see if this customer is gonna ghost us! ğŸ‘»
    """
    A predictive function based on the voting classifier (GBM + RF) findings from the notebook
    This implements a rule-based approach using the actual feature importances
    """
    # Extract features from standardised customer data
    age = customer_data.get('Age', 0)
    balance = customer_data.get('Balance', 0)
    is_active = customer_data.get('IsActiveMember', 0)
    geography_germany = customer_data.get('Geography_Germany', 0)
    geography_spain = customer_data.get('Geography_Spain', 0)
    gender = customer_data.get('Gender', 0)
    num_products = customer_data.get('NumOfProducts', 0)
    credit_score = customer_data.get('CreditScore', 0)
    tenure = customer_data.get('Tenure', 0)
    estimated_salary = customer_data.get('EstimatedSalary', 0)
    has_cr_card = customer_data.get('HasCrCard', 0)
    
    # Base probability
    base_prob = 0.16  # Overall churn rate in dataset
    
    # Apply weights based on feature importance findings from the GBM model
    # Boosting churn probability like an NPC leveling up ğŸ“ˆğŸ®
    churn_prob = base_prob
    
    # Age effect (highest importance feature - 0.3827)
    if age < 30:
        churn_prob += 0.25          # Gen Z be switching banks like it's a TikTok trend ğŸ“±ğŸ”¥
    elif age < 40:
        churn_prob += 0.15          # Millennials? More stable but still got wanderlust ğŸ˜¬
    elif age > 60:
        churn_prob -= 0.20          # Boomers ain't moving â€“ they set for life ğŸ¡ğŸ“œ
    
    # Number of Products effect (second most important - 0.3136)
    if num_products == 1:
        churn_prob += 0.22          # No loyalty, no attachments â€“ just vibes ğŸˆ
    elif num_products >= 3:
        churn_prob -= 0.18          # Multi-product users locked in harder than a fortnite battle pass ğŸ”’ğŸ®
    
    # Active member effect (third most important - 0.1125)
    if is_active == 0:
        churn_prob += 0.15          # Bank account is a ghost town ğŸ‘»ğŸ’€
    
    # Balance effect (fourth most important - 0.0793)
    if balance < 10000:
        churn_prob += 0.08          # Low-balance customers running on financial fumes â›½
    elif balance > 100000:
        churn_prob -= 0.06          # Rich folks staying put, that bank is basically home now ğŸ¦ğŸ’
    
    # Geography effect (Germany - 0.0496)
    if geography_germany == 1:
        churn_prob += 0.05          # Oktoberfest > Banking loyalty ğŸ»ğŸš€
    
    # Credit score effect (0.0213)
    if credit_score < 600:
        churn_prob += 0.02          # This ain't looking good, chief ğŸ’€
    elif credit_score > 750:
        churn_prob -= 0.02          # High credit score? They responsible af ğŸ“Šâœ…
    
    # Estimated Salary effect (0.0208)
    if estimated_salary < 50000:
        churn_prob += 0.02          # They might be looking for better financial deals ğŸ§
    elif estimated_salary > 150000:
        churn_prob -= 0.02          # CEO mindset, staying put ğŸ¢ğŸ•´ï¸
    
    # Gender effect (0.0144)
    if gender == 1:             # Male â€“ Bro energy detected ğŸš¹ğŸ§
        churn_prob += 0.01          # Slightly more likely to dip â€“ classic ğŸš—ğŸ’¨
    
    # Tenure effect (0.0039) â€“ Loyalty be mid ğŸ¤·â€â™‚ï¸
    if tenure <= 2:
        churn_prob += 0.01          # Newbies be outta here ğŸ“¦â¡ï¸ğŸ¦
    elif tenure >= 8:
        churn_prob -= 0.01          # Been here too long to start over ğŸ’¼â³
    
    # Geography Spain effect (0.0009) â€“ Spain without the S ğŸ‡ªğŸ‡¸ğŸ˜Œ
    if geography_spain == 1:
        churn_prob -= 0.001         # Spanish customers just chilling, no rush ğŸ–ï¸ğŸ’ƒ
    
    # Has Credit Card effect (0.0008)
    if has_cr_card == 1:
        churn_prob -= 0.001         # That cashback got them staying fr ğŸ’³ğŸ’–
    
    # Ensure probability is between 0 and 1
    churn_prob = max(min(churn_prob, 0.95), 0.05)
    
    return churn_prob

try:
    df, feature_df = load_data()
    data_loaded = True
except Exception as e:
    st.error(f"Error loading data: {e}")
    data_loaded = False
    # Create sample data for demonstration
    df = pd.DataFrame({
        'CreditScore': [650, 700, 850, 600, 780],
        'Age': [35, 45, 28, 60, 40],
        'Tenure': [5, 8, 1, 10, 3],
        'Balance': [76485.0, 125000.0, 0.0, 50000.0, 80000.0],
        'NumOfProducts': [1, 2, 3, 1, 2],
        'IsActiveMember': [1, 1, 0, 1, 0],
        'EstimatedSalary': [100000.0, 85000.0, 120000.0, 60000.0, 90000.0],
        'Gender_Male': [1, 0, 1, 0, 1],
        'Geography_France': [1, 0, 0, 1, 0],
        'Geography_Germany': [0, 1, 0, 0, 0],
        'Geography_Spain': [0, 0, 1, 0, 1],
        'Exited': [0, 0, 1, 0, 1]
    })

# Page content
if page == "Overview":
    # Welcome to the Overview, where the magic happens! âœ¨
    # Overview page content
    st.header("Project Overview")
    st.write("""
Our implementation features an interactive Streamlit web application that provides comprehensive exploratory data analysis, visualisations, and a predictive interface.
This user-friendly dashboard enables bank personnel to not only identify at-risk customers but also gain deeper insights into churn patterns through intuitive data visualisations spanning demographic, geographic, and financial dimensions.
The application's modular structure allows for examination of feature correlations, customer segments, and key performance indicators that drive decision-making.
             """)
    # Sigma Algorithmics - The A-Team ğŸš€
    st.subheader("Team Members")
    st.write("""
             - **Fatin Nurfarzana Binti Abd Razak**
             - **Nadee Tharanga Hapuarachchige Dona**
             - **Wai Yan Moe**
             - **Yew Yen Bin**
             """)
    # Wai Yan Moe = Aura Farming Dev ğŸ˜
    
    st.subheader("The Problem")
    st.write("""
    Customer churn is a critical challenge for banks. When customers leave, banks lose not only their 
    current business but also future opportunities. Early identification of at-risk customers allows 
    for targeted retention strategies, saving resources and improving customer satisfaction.
    """)
    
    st.subheader("The AI Solution")
    st.write("""
    We've developed a machine learning solution that:
    1. Analyses historical customer data to identify patterns associated with churn
    2. Predicts which current customers are at high risk of leaving
    3. Provides insights into the key factors driving customer decisions
    4. Enables personalised retention strategies based on data-driven predictions
    """)
    
    # Display key metrics in a dashboard-like format
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric(label="Model Accuracy", value="87%")
    with col2:
        st.metric(label="Precision", value="78%")
    with col3:
        st.metric(label="Recall", value="49%")
    with col4:
        st.metric(label="ROC-AUC", value="87%")
    
    # Diagram of solution architecture
    st.subheader("Solution Architecture")
    
    # Time to flex our architecture skills! ğŸ’ª
    # Architecture so clean it belongs in the Louvre (Graphviz go brrr) ğŸ›ï¸
    
    # Define the improved graph with horizontal layout
    architecture_diagram = """
    digraph G {
        graph [nodesep=1.2, ranksep=0.8, splines=ortho, pad=0.5];
        rankdir=LR;  # Left to right layout
  
    node [
        shape=box, 
        style=filled, 
        fontname="Helvetica, Arial, sans-serif",
        fontsize=12,
        height=0.5,
        width=1.8,
        fixedsize=true
    ];
  
    edge [fontname="Helvetica, Arial, sans-serif", fontsize=10];
    
    subgraph cluster_0 {
        label="Data Pipeline";
        style=filled;
        color=lightgrey;
        node [style=filled, color=white, fillcolor="#E5F2FF"];
        
        Data [label="Banking Data"];
        Preprocessing [label="Data Preprocessing"];
        FeatureEngineering [label="Feature Engineering"];
        
        # Create a vertical arrangement within the cluster
        { rank=same; Data }
        { rank=same; Preprocessing }
        { rank=same; FeatureEngineering }
        
        Data -> Preprocessing -> FeatureEngineering;
    }
    
    subgraph cluster_1 {
        label="Model Development";
        style=filled;
        color=lightgrey;
        node [style=filled, color=white, fillcolor="#E6FFE6"];
        
        ModelTraining [label="Model Training"];
        ModelEvaluation [label="Model Evaluation"];
        
        # Create a vertical arrangement within the cluster
        { rank=same; ModelTraining }
        { rank=same; ModelEvaluation }
        
        ModelTraining -> ModelEvaluation;
    }
    
    subgraph cluster_2 {
        label="Deployment";
        style=filled;
        color=lightgrey;
        node [style=filled, color=white, fillcolor="#FFF0E6"];
        
        Prediction [label="Churn Prediction"];
        BusinessInsights [label="Business Insights"];
        
        # Create a vertical arrangement within the cluster
        { rank=same; Prediction }
        { rank=same; BusinessInsights }
        
        Prediction -> BusinessInsights;
    }
    
    FeatureEngineering -> ModelTraining [label="Training Data", fontsize=10, minlen=1];
    ModelEvaluation -> Prediction [label="Trained Model", fontsize=10, minlen=1];
    }
    """

    # Add the architecture diagram
    st.write("""
             Banking Churn Architecture
             """)

    # Use Streamlit's native graphviz rendering
    st.graphviz_chart(architecture_diagram)
    
    st.subheader("How to Use This App")
    st.write("""
    Use the sidebar navigation to explore different aspects of our solution:
    - **Data Exploration**: Visualise and understand the dataset
    - **Model Performance**: See how well our AI model performs
    - **Feature Importance**: Discover which factors influence customer churn the most
    - **Prediction Interface**: Test the model with custom input
    - **Business Insights**: Explore actionable recommendations
    - **Model Code**: Provide a transparent view of the underlying machine learning implementation used in our customer churn prediction system.
    """)
    
    st.subheader("Value for Different Users")
    st.write("""
    - **For Business Analysts:** Understand how the predictions are generated without needing to modify the code
    - **For Technical Teams:** Review the implementation for quality assurance or future modifications
    - **For Academic Purposes:** Study the practical application of machine learning in banking customer retention
    """)

elif page == "Data Exploration":
    # Data Exploration page content
    st.header("Data Exploration")
    
    # Load raw dataset
    @st.cache_data
    def load_raw_data():
        # Raw data coming in hot! ğŸ”¥
        raw_df = pd.read_csv('Churn_Modelling.csv')
        return raw_df
    
    raw_df = load_raw_data()
    
    # Raw dataset overview
    st.subheader("Raw Dataset")
    st.write(f"Raw Dataset Shape: {raw_df.shape[0]} rows and {raw_df.shape[1]} columns")
    
    with st.expander("View Raw Data Sample"):
        st.dataframe(raw_df.head())
    
    # Dataset overview
    st.subheader("Feature Engineered Dataset")
    st.write(f"Dataset Shape: {feature_df.shape[0]} rows and {feature_df.shape[1]} columns")
    
    with st.expander("View Data Sample"):
        st.dataframe(feature_df.head())
    
    # Add tabs for better organisation of visualisations
    eda_tabs = st.tabs(["Overview", "Demographics", "Financial Factors", 
                        "Correlations", "Customer Segments", "Feature Importance"])
    # Tabs on tabs on tabs! ğŸ“š
    
    # 1. OVERVIEW TAB
    with eda_tabs[0]:
        st.subheader("Dataset Overview")
        
        with st.expander("About the Dataset"):
            st.markdown("""
            ### About the Dataset
            
            This dataset contains information about bank customers and their churn status. 
            Key features include:
            
            - **Credit Score**: Customer credit rating
            - **Geography**: Customer's location (France, Germany, Spain)
            - **Gender**: Customer's gender
            - **Age**: Customer's age
            - **Tenure**: Number of years as a customer
            - **Balance**: Account balance
            - **Number of Products**: Number of bank products used
            - **Has Credit Card**: Whether customer has a credit card
            - **Is Active Member**: Whether customer is active
            - **Estimated Salary**: Estimated salary of customer
            - **Credit_Age_Ratio**: Credit score to age ratio (engineered feature)
            - **Tenure_Group**: Customer loyalty segments (New, Developing, Mature, Loyal)
            - **Exited**: Target variable - whether customer left the bank (1) or not (0)
            """)
        
        # Show class imbalance pie chart
        # Class imbalance alert: The churners are giving 'Main Character Syndrome' (need resampling) âš–ï¸ğŸ’…
        col1, col2 = st.columns(2)
        
        with col1:
            # Calculate actual churn rate
            churn_rate = feature_df['Exited'].mean()
            stayed_rate = 1 - churn_rate
            
            # Create pie chart with actual values
            fig = px.pie(
                values=[stayed_rate*100, churn_rate*100], 
                names=['Stayed', 'Churned'], 
                title='Customer Churn Distribution',
                color_discrete_sequence=['royalblue', 'crimson']
            )
            fig.update_traces(textinfo='percent+label')
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.markdown(f"""
            ### Class Distribution
            
            - **Churned Customers**: {(churn_rate*100):.1f}%
            - **Retained Customers**: {(stayed_rate*100):.1f}%
            
            This dataset has an imbalanced class distribution, which was addressed during model training using sampling techniques.
            """)
        
        # Basic statistics of numerical features
        st.subheader("Numerical Features Statistics")
        
        # Select only numerical columns
        numeric_df = feature_df.select_dtypes(include=['number'])
        numeric_df = numeric_df.drop(columns=['Exited'])  # Remove target variable
        
        # Display statistics
        st.dataframe(numeric_df.describe())
        
        # Distribution of a selected numerical feature
        st.subheader("Feature Distribution")
        
        numerical_features = ["CreditScore", "Age", "Balance", "NumOfProducts", 
                             "EstimatedSalary", "Credit_Age_Ratio", "Tenure"]
        selected_feature = st.selectbox(
            "Select a feature to visualise its distribution:", 
            numerical_features,
            key="overview_feature_select"
        )
        
        # Create distribution plot
        fig = px.histogram(
            feature_df, 
            x=selected_feature, 
            color="Exited", 
            marginal="box", 
            opacity=0.7,
            color_discrete_map={0: "royalblue", 1: "crimson"},
            labels={0: "Stayed", 1: "Churned"}
        )
        fig.update_layout(title=f'Distribution of {selected_feature} by Churn Status')
        st.plotly_chart(fig, use_container_width=True)
    
    # 2. DEMOGRAPHICS TAB
    with eda_tabs[1]:
        st.subheader("Demographic Factors and Churn")
        
        # Geographic analysis
        st.markdown("### Geographic Analysis")
        
        # Calculate geographic data from the feature_df
        geo_data = feature_df.groupby("Geography").agg({
            "Exited": "mean",
            "Geography": "count"
        }).rename(columns={"Geography": "Customer_Count"}).reset_index()
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Bar chart comparing churn rates by country
            fig = px.bar(
                geo_data, 
                x="Geography", 
                y="Exited",
                color="Exited",
                color_continuous_scale="BuPu",
                text_auto='.1%',
                title="Churn Rate by Country"
            )
            fig.update_layout(yaxis_title="Churn Rate", xaxis_title="Country")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Add coordinates for map visualisation
            geo_data["lat"] = geo_data["Geography"].map({
                "France": 46.603354,
                "Spain": 40.463667, 
                "Germany": 51.165691
            })
            geo_data["lon"] = geo_data["Geography"].map({
                "France": 1.888334, 
                "Spain": -3.74922, 
                "Germany": 10.451526
            })
            
            # Create map
            fig = px.scatter_geo(
                geo_data,
                lat="lat", 
                lon="lon",
                size="Customer_Count",
                color="Exited",
                hover_name="Geography",
                color_continuous_scale="BuPu",
                projection="natural earth",
                title="Customer Distribution by Country"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Gender analysis
        st.markdown("### Gender Analysis")
        
        gender_churn = feature_df.groupby("Gender")['Exited'].agg(
            ['mean', 'count']).reset_index()
        gender_churn.columns = ['Gender', 'Churn_Rate', 'Count']
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.bar(
                gender_churn, 
                x="Gender", 
                y="Churn_Rate",
                color="Churn_Rate",
                color_continuous_scale="BuPu",
                text_auto='.1%',
                title="Churn Rate by Gender"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.pie(
                gender_churn, 
                values="Count", 
                names="Gender", 
                title="Gender Distribution",
                color_discrete_sequence=px.colors.sequential.Rainbow
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Age analysis
        st.markdown("### Age Analysis")
        
        # Create age groups
        age_bins = [18, 30, 40, 50, 60, 100]
        age_labels = ['18-30', '31-40', '41-50', '51-60', '60+']
        
        # Function to create age group data
        def get_age_group_data(df):
            df_copy = df.copy()
            df_copy['Age_Group'] = pd.cut(df_copy['Age'], bins=age_bins, labels=age_labels)
            age_data = df_copy.groupby('Age_Group').agg({
                'Exited': 'mean',
                'Age_Group': 'count'
            }).rename(columns={'Age_Group': 'Count'}).reset_index()
            return age_data
        
        age_data = get_age_group_data(feature_df)
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.bar(
                age_data, 
                x='Age_Group', 
                y='Exited',
                title='Churn Rate by Age Group',
                color='Exited',
                color_continuous_scale='BuPu',
                text_auto='.1%'
            )
            fig.update_layout(xaxis_title='Age Group', yaxis_title='Churn Rate')
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Age distribution
            fig = px.histogram(
                feature_df, 
                x="Age", 
                color="Exited",
                nbins=20, 
                title="Age Distribution by Churn Status",
                color_discrete_map={0: "royalblue", 1: "crimson"},
            )
            fig.add_vline(
                x=feature_df["Age"].mean(), 
                line_dash="dash", 
                line_color="black",
                annotation_text=f"Mean Age: {feature_df['Age'].mean():.1f}"
            )
            st.plotly_chart(fig, use_container_width=True)
    
    # 3. FINANCIAL FACTORS TAB
    with eda_tabs[2]:
        st.subheader("Financial Factors and Churn")
        
        # Balance analysis
        st.markdown("### Account Balance Analysis")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Box plot of balance by churn status
            fig = px.box(
                feature_df, 
                x="Exited", 
                y="Balance",
                color="Exited",
                points="all",
                title="Balance Distribution by Churn Status",
                color_discrete_map={0: "royalblue", 1: "crimson"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Balance category analysis
            balance_bins = [0, 25000, 75000, 125000, 200000, 250000]
            balance_labels = ['Low', 'Medium-Low', 'Medium', 'Medium-High', 'High']
            
            def get_balance_group_data(df):
                df_copy = df.copy()
                df_copy['Balance_Group'] = pd.cut(df_copy['Balance'], bins=balance_bins, labels=balance_labels)
                balance_churn = df_copy.groupby('Balance_Group')['Exited'].mean().reset_index()
                return balance_churn
            
            balance_data = get_balance_group_data(feature_df)
            
            fig = px.bar(
                balance_data, 
                x='Balance_Group', 
                y='Exited',
                title='Churn Rate by Account Balance',
                color='Exited',
                color_continuous_scale='BuPu',
                text_auto='.1%'
            )
            fig.update_layout(xaxis_title='Balance Category', yaxis_title='Churn Rate')
            st.plotly_chart(fig, use_container_width=True)
        
        # Credit Score Analysis
        st.markdown("### Credit Score Analysis")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Credit score distribution
            fig = px.histogram(
                feature_df, 
                x="CreditScore", 
                color="Exited",
                nbins=20, 
                title="Credit Score Distribution by Churn Status",
                color_discrete_map={0: "royalblue", 1: "crimson"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Credit score bins and churn rate
            credit_bins = [300, 500, 600, 700, 800, 900]
            credit_labels = ['Very Poor', 'Poor', 'Fair', 'Good', 'Excellent']
            
            def get_credit_group_data(df):
                df_copy = df.copy()
                df_copy['Credit_Group'] = pd.cut(df_copy['CreditScore'], bins=credit_bins, labels=credit_labels)
                credit_churn = df_copy.groupby('Credit_Group')['Exited'].mean().reset_index()
                return credit_churn
            
            credit_data = get_credit_group_data(feature_df)
            
            fig = px.bar(
                credit_data, 
                x='Credit_Group', 
                y='Exited',
                title='Churn Rate by Credit Score',
                color='Exited',
                color_continuous_scale='BuPu',
                text_auto='.1%'
            )
            fig.update_layout(xaxis_title='Credit Score Category', yaxis_title='Churn Rate')
            st.plotly_chart(fig, use_container_width=True)
        
        # Products and Activity Analysis
        st.markdown("### Products and Activity Analysis")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Number of products analysis
            product_churn = feature_df.groupby("NumOfProducts")['Exited'].mean().reset_index()
            
            fig = px.bar(
                product_churn, 
                x="NumOfProducts", 
                y="Exited",
                title="Churn Rate by Number of Products",
                color="Exited",
                color_continuous_scale="BuPu",
                text_auto='.1%'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Activity status analysis
            activity_churn = feature_df.groupby("IsActiveMember")['Exited'].mean().reset_index()
            activity_churn['Status'] = activity_churn['IsActiveMember'].map({0: 'Inactive', 1: 'Active'})
            
            fig = px.bar(
                activity_churn, 
                x="Status", 
                y="Exited",
                title="Churn Rate by Activity Status",
                color="Exited",
                color_continuous_scale="BuPu",
                text_auto='.1%'
            )
            st.plotly_chart(fig, use_container_width=True)
    
    # 4. CORRELATIONS TAB
    with eda_tabs[3]:
        st.subheader("Feature Correlations")
        
        # Correlation heatmap
        st.markdown("### Correlation Heatmap")
        
        # Select only numerical columns
        numeric_df = feature_df.select_dtypes(include=['number'])
        corr = numeric_df.corr()
        
        fig = px.imshow(
            corr, 
            text_auto='.2f', 
            aspect="auto", 
            color_continuous_scale='BuPu'
        )
        fig.update_layout(title='Feature Correlation Heatmap')
        st.plotly_chart(fig, use_container_width=True)
        
        # Feature correlation with churn
        st.markdown("### Correlation with Churn")
        
        churn_corr = corr["Exited"].drop("Exited").sort_values(ascending=False)
        
        fig = px.bar(
            x=churn_corr.values,
            y=churn_corr.index,
            orientation='h',
            color=churn_corr.values,
            color_continuous_scale='BuPu',
            title='Feature Correlation with Churn'
        )
        fig.update_layout(xaxis_title='Correlation Coefficient', yaxis_title='Feature')
        fig.add_vline(x=0, line_dash="dash", line_color="gray")
        st.plotly_chart(fig, use_container_width=True)
        
        # Scatter plot matrix (optional and computationally intensive)
        st.markdown("### Feature Relationships")
        st.write("Select features to visualise their relationships:")
        
        # Allow users to select features for the scatter matrix
        if st.checkbox("Show scatter plot matrix (may take time to load)"):
            # Select subset of features for pair plot
            pair_features = st.multiselect(
                "Select features for scatter matrix (3-4 recommended):",
                options=numerical_features,
                default=numerical_features[:3]
            )
            
            if pair_features:
                # Create scatter matrix
                fig = px.scatter_matrix(
                    feature_df, 
                    dimensions=pair_features,
                    color="Exited", 
                    opacity=0.7,
                    color_discrete_map={0: "royalblue", 1: "crimson"}
                )
                fig.update_layout(title='Scatter Plot Matrix')
                st.plotly_chart(fig, use_container_width=True)
    
    # 5. CUSTOMER SEGMENTS TAB
    with eda_tabs[4]:
        st.subheader("Customer Segmentation Analysis")
        
        # Tenure group analysis
        st.markdown("### Customer Loyalty Analysis")
        
        tenure_churn = feature_df.groupby('Tenure_Group')['Exited'].mean().reset_index()
        
        fig = px.bar(
            tenure_churn, 
            x='Tenure_Group', 
            y='Exited',
            title='Churn Rate by Tenure Group',
            color='Exited',
            color_continuous_scale='BuPu',
            text_auto='.1%'
        )
        fig.update_layout(xaxis_title='Tenure Group', yaxis_title='Churn Rate')
        st.plotly_chart(fig, use_container_width=True)
        
        # 3D visualisation of customer segments
        st.markdown("### 3D Visualisation of Customer Segments")
        
        # Allow user to select features for 3D plot
        x_feature = st.selectbox("Select X-axis feature:", numerical_features, index=0, key="x_feature")
        y_feature = st.selectbox("Select Y-axis feature:", numerical_features, index=1, key="y_feature")
        z_feature = st.selectbox("Select Z-axis feature:", numerical_features, index=2, key="z_feature")
        
        # Create 3D scatter plot
        fig = px.scatter_3d(
            feature_df, 
            x=x_feature,
            y=y_feature,
            z=z_feature,
            color='Exited',
            opacity=0.7,
            color_discrete_map={0: "royalblue", 1: "crimson"}
        )
        fig.update_layout(title=f'3D Scatter Plot of Customer Segments')
        st.plotly_chart(fig, use_container_width=True)
        
        # Customer profiles
        st.markdown("### Customer Profiles")
        
        st.markdown("""
        Based on our analysis, we can identify several key customer profiles:
        
        1. **High-Risk Customers**:
           - Younger customers (under 30 years)
           - Customers with single product
           - Inactive members
           - Customers with low balances
           - German customers
        
        2. **Low-Risk Customers**:
           - Older customers (40+ years)
           - Customers with multiple products (3+)
           - Active account holders
           - Customers with higher balances
           - French customers
        
        These insights can help in designing targeted retention strategies for different customer segments.
        """)
    
    # 6. FEATURE IMPORTANCE TAB - UPDATED (version 2)
    # Features ranked by slay factor ğŸ’…âœ¨ (spoiler: Age ate and left no crumbs)
    with eda_tabs[5]:
        st.subheader("Feature Importance Analysis")
        
        # Feature importance from model
        st.markdown("### Feature Importance from Model")
        
        # Create feature importance dataframe
        feature_importance_sorted = pd.DataFrame({
            'Feature': list(feature_importance.keys()),
            'Importance': list(feature_importance.values())
        }).sort_values('Importance', ascending=False)
        
        fig = px.bar(
            feature_importance_sorted,
            x='Importance',
            y='Feature',
            orientation='h',
            color='Importance',
            color_continuous_scale='BuPu',
            title='Feature Importance Ranking'
        )
        fig.update_layout(yaxis={'categoryorder':'total ascending'})
        st.plotly_chart(fig, use_container_width=True)
        
        # Feature impact interpretation
        st.markdown("### Feature Impact Interpretation")
        
        st.markdown('''
        ### Key Drivers of Customer Churn:
        
        1. **Age** (38.3% importance):
           - Younger customers tend to churn more frequently
           - They often switch financial service providers in search of better deals
        
        2. **Number of Products** (31.4% importance):
           - Customers with fewer products show higher churn rates
           - Multiple products create stronger banking relationships and "stickiness"
        
        3. **Activity Status** (11.3% importance):
           - Inactive customers have significantly higher churn rates
           - Regular engagement is critical for retention
        
        4. **Balance** (7.9% importance):
           - Lower balance customers may see limited benefits from banking services
           - Financial instability may correlate with higher churn probability
        
        5. **Geography** (5.0% importance for Germany):
           - German customers churn at higher rates
           - Regional banking preferences and market competition may contribute
        
        These insights can guide targeted interventions to improve customer retention.
        ''')
        
        # Model predictions vs. actual outcomes
        st.markdown("### Model Performance by Feature Segments")
        
        st.markdown('''
        The model performs best at identifying:
        
        1. Younger customers with single products
        2. Inactive customers with low balances
        3. Customers from specific geographic regions (e.g., Germany)
        
        The model is less accurate at predicting churn for:
        
        1. Customers with multiple products and high engagement
        2. Customers with long tenure and stable financial profiles
        3. Customers with average feature values across multiple dimensions
        
        These insights help prioritise which customers to focus retention efforts on.
        ''')
        
   
elif page == "Model Performance":
    # Model Performance page content
    st.header("Model Performance")
    
    # Show the model selection process
    # The Training Arc begins (Cue the montage music) ğŸ¥ŠğŸ‹ï¸â€â™‚ï¸
    st.subheader("Model Selection Process")
    st.write("""
    Our team evaluated multiple machine learning models to find the best approach for predicting customer churn.
    We tested 8 different models with various resampling techniques to address class imbalance:
    
    - Logistic Regression with L2 regularisation
    - K-Nearest Neighbors (k=7)
    - Decision Trees (CART)
    - Gaussian Naive Bayes
    - Support Vector Machine with linear kernel
    - Gradient Boosting Machine (GBM)
    - Random Forest (RF) with Gini criterion
    - Multi-Layer Perceptron (Neural Network)
    
    After rigorous evaluation, the best performing models were GBM and Random Forest. 
    The final model is a voting classifier that combines these two models for optimal performance.
    """)
    
    # Show model evaluation chart
    st.subheader("Model Comparison")
    
    model_comparison = pd.DataFrame({
        'Model': ['CART', 'RF_Gini100', 'MLP', 'GBM', 'Voting Classifier'],
        'Accuracy': [0.82, 0.85, 0.84, 0.86, 0.87],
        'Precision': [0.72, 0.75, 0.74, 0.77, 0.78],
        'Recall': [0.43, 0.46, 0.45, 0.47, 0.49],
        'ROC-AUC': [0.82, 0.85, 0.84, 0.86, 0.87]
    })
    
    fig = px.bar(model_comparison, x='Model', y=['Accuracy', 'Precision', 'Recall', 'ROC-AUC'], 
                 barmode='group', title='Model Performance Metrics')
    st.plotly_chart(fig, use_container_width=True)
    
    # Code snippet showing final model implementation
    with st.expander("View Final Model Implementation"):
        st.code("""
# Final Voting Classifier Implementation
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score

# Initialise the top 2 models (GBM and RF)
rf = RandomForestClassifier(
    n_estimators=100,
    criterion='gini',
    random_state=42
)

gbm = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.15,
    max_depth=4,
    random_state=42
)

# Create the Voting Classifier with GBM and RF
voting_clf = VotingClassifier(
    estimators=[
        ('rf', rf),
        ('gbm', gbm)
    ],
    voting='soft'  # Use soft voting to average predicted probabilities
)

# Define the hyperparameter grid for weighting the models
param_grid = {
    'weights': [
        [1, 1],  # Equal weight for RF and GBM
        [2, 1],  # Emphasise Random Forest
        [1, 2],  # Emphasise GBM
        [3, 2]   # Custom weight combination favoring RF
    ]
}

# Define K-Fold Cross-Validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# Perform Grid Search with Cross-Validation
grid_search = GridSearchCV(
    estimator=voting_clf,
    param_grid=param_grid,
    scoring='roc_auc',
    cv=kf,
    verbose=1,
    n_jobs=-1
)

# Train the model
grid_search.fit(X_train, y_train)

# Use the best ensemble model found by GridSearchCV
best_voting_clf = grid_search.best_estimator_

# Make predictions
y_pred = best_voting_clf.predict(X_test)
y_proba = best_voting_clf.predict_proba(X_test)[:, 1]

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
        """, language="python")
    
    # Display model metrics
    st.subheader("Performance Metrics")
    metrics = {
        "Accuracy": 0.87,
        "Precision": 0.78,
        "Recall": 0.49,
        "F1-Score": 0.60,
        "ROC-AUC": 0.87
    }
    
    # Metrics visualisation
    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=list(metrics.keys()),
        y=list(metrics.values()),
        marker_color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    ))
    fig.update_layout(
        title='Model Performance Metrics',
        yaxis_title='Score',
        yaxis=dict(range=[0, 1])
    )
    st.plotly_chart(fig, use_container_width=True)
    
    # Confusion Matrix
    # Confusion Matrix: Checking if the model is imposter or crewmate à¶ğŸ•µï¸â€â™‚ï¸
    st.subheader("Confusion Matrix")
    conf_matrix = np.array([[1700, 300], [200, 200]])  # Example values - replace with actual
    
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_xlabel('Predicted Label')
    ax.set_ylabel('True Label')
    ax.set_title('Confusion Matrix')
    ax.set_xticklabels(['Not Churned', 'Churned'])
    ax.set_yticklabels(['Not Churned', 'Churned'])
    st.pyplot(fig)
    
    # ROC Curve
    # Model's rizz curve ğŸ“ˆ - better than my DMs
    st.subheader("ROC Curve")
    fpr = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])
    tpr = np.array([0, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.98, 1.0])
    
    fig = px.area(
        x=fpr, y=tpr,
        title=f'ROC Curve (AUC={metrics["ROC-AUC"]})',
        labels=dict(x='False Positive Rate', y='True Positive Rate'),
        width=700, height=500
    )
    fig.add_shape(
        type='line', line=dict(dash='dash'),
        x0=0, x1=1, y0=0, y1=1
    )
    fig.update_layout(template='plotly_white')
    st.plotly_chart(fig)
    
    # Model comparison
    st.subheader("Model Comparison")
    models = ['Logistic Regression', 'Random Forest', 'GBM', 'Voting Classifier (Final)']
    accuracy = [0.82, 0.85, 0.86, 0.87]
    precision = [0.72, 0.75, 0.77, 0.78]
    recall = [0.42, 0.45, 0.47, 0.49]
    
    comparison_df = pd.DataFrame({
        'Model': models,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall
    })
    
    fig = px.bar(comparison_df, x='Model', y=['Accuracy', 'Precision', 'Recall'], 
                barmode='group', title='Model Comparison')
    st.plotly_chart(fig, use_container_width=True)
    
    # Evaluation summary
    st.subheader("Evaluation Summary")
    st.write("""
    Our final voting classifier model that combines GBM and Random Forest achieves a good balance 
    of precision and recall. With 87% accuracy, the model correctly classifies most customers.
    
    The precision of 78% indicates that when our model predicts a customer will churn, it's right 
    78% of the time. This helps focus retention efforts efficiently.
    
    The recall of 49% means we're capturing about half of all customers who will actually churn. 
    While there's room for improvement, this still represents significant business value by enabling 
    proactive retention for many at-risk customers.
    
    The high ROC-AUC score of 87% demonstrates the model's strong ability to distinguish between 
    churners and non-churners.
    """)

elif page == "Feature Importance":
    # Feature Importance page content - UPDATED (version 2)
    st.header("Feature Importance Analysis")
    
    # Feature importance from the trained GBM model in the notebook
    # Create feature importance dataframe
    importance_df = pd.DataFrame({
        'Feature': list(feature_importance.keys()),
        'Importance': list(feature_importance.values())
    }).sort_values('Importance', ascending=False)
    
    # Bar chart of feature importance
    fig = px.bar(importance_df, x='Importance', y='Feature', orientation='h',
                color='Importance', title='Feature Importance Ranking')
    st.plotly_chart(fig, use_container_width=True)
    
    # Feature importance interpretation
    st.subheader("Key Drivers of Customer Churn")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### Top Factors
        
        1. **Age** (38.3%)
           * Younger customers tend to churn more frequently
           * They often switch financial service providers in search of better deals
        
        2. **Number of Products** (31.4%)
           * Customers with fewer products show higher churn rates
           * Multiple products create stronger banking relationships and "stickiness"
        
        3. **Activity Status** (11.3%)
           * Inactive customers have significantly higher churn rates
           * Regular engagement is critical for retention
        """)
    
    with col2:
        st.markdown("""
        ### Additional Insights
        
        4. **Balance** (7.9%)
           * Lower balance customers may see limited benefits from banking services
           * Financial instability may correlate with higher churn probability
        
        5. **Geography** (5.0% for Germany)
           * German customers churn at higher rates
           * Regional banking preferences and market competition may contribute
        
        6. **Credit Score** (2.1%)
           * Lower impact on churn decisions compared to top factors
           * Still provides relevant signals in combination with other features
        """)
    
    # Model agreement on feature selection
    st.subheader("Model Agreement on Feature Selection")
    
    # Example data showing how many models selected each feature
    agreement_data = {
        'Feature': ['Age', 'NumOfProducts', 'Balance', 'IsActiveMember', 'EstimatedSalary', 
                   'CreditScore', 'Geography_Germany', 'Gender', 'Tenure'],
        'Selection Frequency': [4, 4, 4, 3, 3, 3, 2, 2, 1]
    }
    agreement_df = pd.DataFrame(agreement_data)
    
    fig = px.bar(agreement_df, x='Feature', y='Selection Frequency', 
                title='Feature Selection Frequency Across Methods',
                color='Selection Frequency')
    st.plotly_chart(fig, use_container_width=True)
    
    # Feature correlations visualisation
    st.subheader("Feature Correlations with Churn")
    
    # Create sample correlation data
    feature_correlation = {
        'Feature': ['Age', 'NumOfProducts', 'IsActiveMember', 'Balance', 
                   'Geography_Germany', 'CreditScore', 'Gender_Male', 'Tenure'],
        'Correlation': [0.28, -0.18, -0.32, 0.23, 0.15, -0.12, 0.03, -0.09]
    }
    corr_df = pd.DataFrame(feature_correlation).sort_values('Correlation')
    
    fig = px.bar(corr_df, x='Correlation', y='Feature', orientation='h',
                title='Feature Correlation with Churn',
                color='Correlation', color_continuous_scale='RdBu_r')
    st.plotly_chart(fig, use_container_width=True)

elif page == "Prediction Interface":
    # Prediction Interface page content
    # *slams predict button* "SHOW ME THE CHURNED!!" ğŸ“‰
    st.header("Churn Prediction Interface")
    st.write("Enter customer information to predict their likelihood of churning")
    
    # Create input form
    col1, col2 = st.columns(2)
    
    with col1:
        credit_score = st.slider("Credit Score", 300, 850, 650)
        age = st.slider("Age", 18, 100, 35)
        tenure = st.slider("Tenure (years)", 0, 10, 5)
        balance = st.number_input("Balance", min_value=0.0, max_value=250000.0, value=76485.0, step=1000.0)
        num_products = st.slider("Number of Products", 1, 4, 1)
    
    with col2:
        geography = st.selectbox("Geography", ["France", "Germany", "Spain"])
        gender = st.radio("Gender", ["Male", "Female"])
        has_credit_card = st.checkbox("Has Credit Card")
        is_active_member = st.checkbox("Is Active Member")
        estimated_salary = st.number_input("Estimated Salary", min_value=0.0, max_value=200000.0, value=100000.0, step=1000.0)
    
    # Calculate Credit_Age_Ratio
    credit_age_ratio = credit_score / age if age > 0 else 0
    
    # Additional inputs based on notebook feature engineering
    tenure_group_loyal = st.checkbox("Is a Loyal Customer (8+ years)", value=True if tenure >= 8 else False)
    
    # Make prediction on button click
    if st.button("Predict Churn Probability"):
        # Calculating the 'Ghost Probability' (Churn Risk) ğŸ‘»ğŸ“‰
        # Convert inputs to the format expected by our prediction function
        customer_data = {
            'Age': age,
            'Balance': balance,
            'IsActiveMember': 1 if is_active_member else 0,
            'NumOfProducts': num_products,
            'CreditScore': credit_score,
            'Geography_Germany': 1 if geography == "Germany" else 0,
            'Geography_Spain': 1 if geography == "Spain" else 0,
            'Gender': 1 if gender == "Male" else 0,
            'Tenure': tenure,
            'EstimatedSalary': estimated_salary,
            'HasCrCard': 1 if has_credit_card else 0,
            'Credit_Age_Ratio': credit_age_ratio,
            'Tenure_Group_Loyal': 1 if tenure_group_loyal else 0,
            'Tenure_Group_Developing': 0,  # Added for completeness
            'Tenure_Group_Mature': 0  # Added for completeness
        }
        
        # Use our predictive function based on the actual model
        churn_probability = predict_churn(customer_data)
        
        # Display prediction
        st.subheader("Prediction Result")
        
        col1, col2 = st.columns([1, 3])
        with col1:
            # Create a gauge chart for the probability
            fig = go.Figure(go.Indicator(
                mode = "gauge+number",
                value = churn_probability * 100,
                domain = {'x': [0, 1], 'y': [0, 1]},
                title = {'text': "Churn Probability"},
                gauge = {
                    'axis': {'range': [0, 100]},
                    'bar': {'color': "darkred"},
                    'steps': [
                        {'range': [0, 30], 'color': "green"},
                        {'range': [30, 70], 'color': "yellow"},
                        {'range': [70, 100], 'color': "red"}
                    ],
                    'threshold': {
                        'line': {'color': "black", 'width': 4},
                        'thickness': 0.75,
                        'value': churn_probability * 100
                    }
                }
            ))
            st.plotly_chart(fig)
            
        with col2:
            # Interpretation and recommendation
            if churn_probability < 0.3:
                risk_level = "Low Risk"
                st.success(f"Churn Risk: {risk_level}")
                st.write("""
                **Recommendation:** This customer appears satisfied with their banking relationship. Consider:
                - Maintaining the current level of service
                - Exploring opportunities for product upselling
                - Including in general marketing campaigns
                """)
            elif churn_probability < 0.7:
                risk_level = "Medium Risk"
                st.warning(f"Churn Risk: {risk_level}")
                st.write("""
                **Recommendation:** This customer shows some warning signs. Consider:
                - Proactive outreach to assess satisfaction
                - Offering product enhancements or loyalty benefits
                - Reviewing their account activity for potential pain points
                """)
            else:
                risk_level = "High Risk"
                st.error(f"Churn Risk: {risk_level}")
                st.write("""
                **Recommendation:** This customer is at high risk of leaving. Consider:
                - Immediate personalised retention offer
                - Direct contact from account manager
                - Tailored solutions to address likely pain points
                - Special loyalty program enrollment
                """)

elif page == "Business Insights":
    # Time to drop some knowledge bombs! ğŸ’£
    # Business Insights page content - UPDATED (version 2)
    # CEO of customer retention strategy ğŸ’¼ğŸ§ 
    st.header("Business Insights and Recommendations")
    
    # Key Findings
    st.subheader("Key Findings")
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        ğŸ“Š **Customer Segmentation**
        - Age is the strongest predictor of churn (38.3%) - younger customers switch providers more frequently
        - Customers with fewer products have weaker banking relationships and higher churn rates
        - Inactive members are significantly more likely to leave
        - Low-balance customers show higher churn probability
        - Geographic location (particularly Germany) influences churn probability
        """)
    
    with col2:
        st.markdown("""
        ğŸ’¡ **Product Insights**
        - Single-product customers are most vulnerable to churn
        - Cross-selling additional products substantially reduces churn risk
        - Active account usage and regular transactions indicate stronger customer retention
        - Digital engagement correlates with lower churn rates
        - Financial stability indicators (higher balances, credit scores) correlate with retention
        """)
    
    # Actionable Recommendations
    st.subheader("Actionable Recommendations")
    st.markdown("""
                Based on the model's predictions and interactive data exploration, the following segment-specific strategies are recommended:
                """)
    
    tab1, tab2, tab3 = st.tabs(["For High-Risk Customers", "For Medium-Risk Customers", "For Low-Risk Customers"])

# When they're 'bout to ghost harder than my crush ğŸ’”ğŸ“‰ â€“ deploy the EMERGENCY RIZZ PACKAGE ğŸš¨ğŸ· (discounts, freebies, *puppy eyes*)
    with tab1:
        st.markdown("""
        ### High-Risk Customer Strategy
        
        - **Age-Based Targeting**: Develop specialised retention programs for younger customers (under 30) with personalised offers that align with their financial goals and digital preferences.

        - **Product Expansion**: Launch targeted cross-selling campaigns for single-product customers, emphasising benefits of product bundling and relationship rewards.

        - **Re-Engagement Campaigns**: Implement dedicated reactivation strategies for inactive customers, including special incentives for renewed transaction activity.
        
        - **Low Balance Initiatives**: Create tailored value propositions for customers with lower account balances, such as fee waivers or enhanced digital banking features.
        
        - **Geographic Customisation**: Address specific needs of German customers with localised marketing and service enhancements to counteract regional competitive pressures.
        """)

# Customers giving side eye ğŸ‘€ â€“ hit 'em with "rizz-light mode" ğŸ’¡âœ¨ (mid rizz = free trial + "u up?" text vibes)        
    with tab2:
        st.markdown("""
        ### Medium-Risk Customer Strategy
        
        - **Relationship Deepening**: Enhance existing customer relationships through targeted financial advisory services and periodic account reviews.

        - **Engagement Milestones**: Create interaction-based loyalty programs that reward consistent engagement and account activity.
        
        - **Product Bundling**: Introduce tailored product bundles designed to increase customer ties to the bank while providing genuine value.
        
        - **Early Warning System**: Implement monitoring for declining transaction patterns or engagement metrics to enable proactive intervention.
        """)

# The Gigachads of Loyalty ğŸ—¿ğŸ‘‘ â€“ Retention rates looking immaculate ğŸ’¯ğŸ”¥      
    with tab3:
        st.markdown("""
        ### Low-Risk Customer Strategy
        
        - **Loyalty Recognition**: Strengthen existing relationships through targeted appreciation campaigns and exclusive benefits for long-term customers.

        - **Ambassador Programs**: Leverage satisfied customers through referral programs that reward customer advocacy.
        
        - **Premium Service Tiers**: Introduce enhanced service levels for customers with strong retention indicators to further solidify their banking relationship.
        
        - **Proactive Cross-Selling**: Strategically expand product relationships based on sophisticated next-best-product analytics.
        """)
    
    st.markdown("""
                By utilising these data-driven insights and the interactive visualisation platform, bank staff can implement precise interventions that optimise resource allocation while simultaneously improving customer satisfaction and profitability.
                """)
    
    # Expected Impact
    st.subheader("Expected Impact")
    
    impact_data = {
        'Metric': ['Customer Retention Rate', 'Customer Lifetime Value', 'Acquisition Cost Savings', 'Cross-Sell Success Rate'],
        'Current': [0.80, 10000, 0, 0.15],
        'Expected': [0.88, 12500, 1200000, 0.22],
        'Improvement': ['+10%', '+25%', '$1.2M', '+47%']
    }
    
    impact_df = pd.DataFrame(impact_data)
    st.table(impact_df)
    
    # Implementation Roadmap
    st.subheader("Implementation Roadmap")
    
    roadmap_data = {
        'Phase': ['Immediate (1-30 days)', 'Short-term (1-3 months)', 'Medium-term (3-6 months)', 'Long-term (6-12 months)'],
        'Actions': [
            'Identify and contact top 100 high-risk customers',
            'Deploy automated risk scoring and alerting system',
            'Roll out comprehensive retention program',
            'Establish continuous monitoring and optimisation'
        ]
    }
    
    roadmap_df = pd.DataFrame(roadmap_data)
    st.table(roadmap_df)

# New "Model Code" page to showcase the team's code
# Peek behind the curtain, here's the code! ğŸ‘€
elif page == "Model Code":
    st.header("Model Code and Implementation")
    
    # Code tabs for different parts of the model development
    code_tabs = st.tabs(["Data Cleaning", "Feature Engineering", "Feature Selection", "Model Training", "Model Evaluation"])
    
    with code_tabs[0]:
        st.subheader("Data Cleaning and Preprocessing")
        st.code("""
# Example of data cleaning and preprocessing
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('Churn_Modelling.csv')

# Check for missing values
print(df.isnull().sum())

# Basic data exploration
print(df.describe())

# Data cleaning
# Remove unnecessary columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# Handle categorical variables
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=False)

# Check for outliers
# Example: Credit Score should be between 300-850
outliers = df[(df['CreditScore'] < 300) | (df['CreditScore'] > 850)]
print(f"Number of outliers in CreditScore: {len(outliers)}")

# Handle outliers (cap method)
df['CreditScore'] = np.where(df['CreditScore'] < 300, 300, df['CreditScore'])
df['CreditScore'] = np.where(df['CreditScore'] > 850, 850, df['CreditScore'])

# Save cleaned data
df.to_csv('cleaned_data.csv', index=False)
        """, language="python")
    
    with code_tabs[1]:
        st.subheader("Feature Engineering")
        st.code("""
# Feature Engineering Example
import pandas as pd
import numpy as np

# Load cleaned data
df = pd.read_csv('cleaned_data.csv')

# Create new features

# 1. Balance per Product ratio
df['BalancePerProduct'] = df['Balance'] / (df['NumOfProducts'] + 0.1)  # Adding 0.1 to avoid division by zero

# 2. Balance to Salary ratio
df['BalanceToSalaryRatio'] = df['Balance'] / (df['EstimatedSalary'] + 0.1)

# 3. Credit score bins
df['CreditScoreBin'] = pd.cut(
    df['CreditScore'], 
    bins=[300, 580, 670, 740, 800, 850], 
    labels=['Very Poor', 'Poor', 'Fair', 'Good', 'Excellent']
)

# 4. Age groups
df['AgeGroup'] = pd.cut(
    df['Age'], 
    bins=[18, 30, 40, 50, 60, 100], 
    labels=['18-30', '31-40', '41-50', '51-60', '60+']
)

# 5. Customer Engagement Score
df['EngagementScore'] = (
    (df['IsActiveMember'] * 0.5) + 
    (df['HasCrCard'] * 0.3) + 
    (np.log1p(df['Tenure']) * 0.2)  # Log transform to reduce impact of outliers
)

# 6. Interaction terms
df['Age_Balance_Interaction'] = df['Age'] * np.log1p(df['Balance'])
df['Active_Products_Interaction'] = df['IsActiveMember'] * df['NumOfProducts']

# Save feature engineered data
df.to_csv('feature_engineered_data.csv', index=False)
        """, language="python")
    
    with code_tabs[2]:
        st.subheader("Feature Selection")
        st.code("""
# Feature Selection Example
import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler

# Load data
df = pd.read_csv('feature_engineered_data.csv')

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# 1. Handle categorical variables
# Convert categorical to numerical
X = pd.get_dummies(X, drop_first=True)

# 2. Check for multicollinearity with VIF
def calculate_vif(df):
    vif_data = pd.DataFrame()
    vif_data["Feature"] = df.columns
    vif_data["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    return vif_data.sort_values('VIF', ascending=False)

# Standardise features for VIF calculation
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

vif = calculate_vif(X_scaled)
print("VIF before feature removal:")
print(vif)

# Iteratively remove highest VIF feature until threshold is met
vif_threshold = 5.0
X_vif = X.copy()

while True:
    vif = calculate_vif(X_vif)
    if vif['VIF'].max() <= vif_threshold:
        break
    highest_vif_feature = vif.iloc[0]['Feature']
    print(f"Removing {highest_vif_feature} with VIF {vif['VIF'].max()}")
    X_vif = X_vif.drop(highest_vif_feature, axis=1)

print(f"Remaining features after VIF filtering: {X_vif.columns.tolist()}")

# 3. Statistical Feature Selection (ANOVA F-test)
selector = SelectKBest(f_classif, k=10)
X_new = selector.fit_transform(X, y)
selected_mask = selector.get_support()
selected_features = X.columns[selected_mask]
print(f"Features selected by ANOVA F-test: {selected_features.tolist()}")

# 4. Model-based Feature Selection (Random Forest)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False)
print("Random Forest Feature Importance:")
print(feature_importances.head(10))

# 5. Recursive Feature Elimination
rfe = RFE(estimator=RandomForestClassifier(n_estimators=10, random_state=42), n_features_to_select=10)
rfe.fit(X, y)
selected_features_rfe = X.columns[rfe.support_]
print(f"Features selected by RFE: {selected_features_rfe.tolist()}")

# Combine feature selection methods
feature_selection_methods = {
    'VIF': set(X_vif.columns),
    'ANOVA': set(selected_features),
    'RF': set(feature_importances.nlargest(10, 'Importance')['Feature']),
    'RFE': set(selected_features_rfe)
}

# Count how many methods selected each feature
feature_counts = {}
for feature in X.columns:
    count = sum(1 for method_features in feature_selection_methods.values() if feature in method_features)
    feature_counts[feature] = count

# Select features that appear in at least 2 methods
final_features = [feature for feature, count in feature_counts.items() if count >= 2]
print(f"Final selected features: {final_features}")

# Save the selected features
X_selected = X[final_features]
final_df = pd.concat([X_selected, y], axis=1)
final_df.to_csv('cleaned_data_with_target.csv', index=False)
        """, language="python")
    
    with code_tabs[3]:
        st.subheader("Model Training")
        st.code("""
# Model Training Example
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the data with selected features
df = pd.read_csv('cleaned_data_with_target.csv')

# Separate features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train multiple models

# 1. Logistic Regression
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train, y_train)

# 2. Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 3. Gradient Boosting Machine
gbm = GradientBoostingClassifier(n_estimators=100, random_state=42)
gbm.fit(X_train, y_train)

# Evaluate the models
models = {
    'Logistic Regression': lr,
    'Random Forest': rf,
    'Gradient Boosting': gbm
}

for name, model in models.items():
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    
    print(f"Model: {name}")
    print(f"Accuracy: {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall: {rec:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC-AUC: {auc:.4f}")
    print("-" * 40)

# Select top 2 models for ensemble (assuming RF and GBM performed best)
# Hyperparameter tuning for the ensemble
param_grid = {
    'rf__n_estimators': [50, 100, 200],
    'rf__max_depth': [None, 10, 20],
    'gbm__n_estimators': [50, 100, 200],
    'gbm__learning_rate': [0.05, 0.1, 0.2]
}

# Create voting classifier
voting_clf = VotingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(random_state=42)),
        ('gbm', GradientBoostingClassifier(random_state=42))
    ],
    voting='soft'
)

# Grid search for best parameters
grid_search = GridSearchCV(
    voting_clf, param_grid, cv=5, scoring='roc_auc',
    n_jobs=-1, verbose=1
)
grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")

# Evaluate the final model
final_model = grid_search.best_estimator_
y_pred = final_model.predict(X_test)
y_prob = final_model.predict_proba(X_test)[:, 1]

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_prob)

print(f"Final Model (Voting Classifier)")
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall: {rec:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC-AUC: {auc:.4f}")

# Save the final model
import pickle
with open('final_model.pkl', 'wb') as f:
    pickle.dump(final_model, f)
        """, language="python")
    
    with code_tabs[4]:
        st.subheader("Model Evaluation")
        st.code("""
# Model Evaluation Example
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import pickle

# Load the model and test data
with open('final_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Load data
df = pd.read_csv('final_selected_features.csv')

# Split the data
from sklearn.model_selection import train_test_split
X = df.drop('Exited', axis=1)
y = df['Exited']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Make predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# 1. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.savefig('confusion_matrix.png')
plt.close()

# 2. Classification Report
cr = classification_report(y_test, y_pred, output_dict=True)
cr_df = pd.DataFrame(cr).transpose()
print("Classification Report:")
print(cr_df)

# 3. ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.savefig('roc_curve.png')
plt.close()

# 4. Feature Importance
# Extract feature importance from the GBM component of the voting classifier
gbm_importances = model.named_estimators_['gbm'].feature_importances_
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': gbm_importances
}).sort_values('Importance', ascending=False)

print("GBM Feature Importance:")
print(feature_importance)

# Save feature importance
feature_importance.to_csv('feature_importance.csv', index=False)

# 5. Finding optimal threshold
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)

# Create DataFrame for analysis
threshold_df = pd.DataFrame({
    'Threshold': [0] + list(thresholds),
    'Precision': list(precisions),
    'Recall': list(recalls)
})

# Calculate F1 Score
threshold_df['F1_Score'] = 2 * (threshold_df['Precision'] * threshold_df['Recall']) / (threshold_df['Precision'] + threshold_df['Recall'])

# Find threshold with highest F1 score
best_threshold = threshold_df.loc[threshold_df['F1_Score'].idxmax(), 'Threshold']
print(f"Best threshold: {best_threshold:.4f}")

# 6. Business impact analysis
# Cost of false positive (implementing retention on customer who wouldn't churn) = $100
# Cost of false negative (losing a customer) = $1000
threshold_df['FP_Rate'] = [sum((y_prob >= t) & (y_test == 0)) / sum(y_test == 0) for t in threshold_df['Threshold']]
threshold_df['FN_Rate'] = [sum((y_prob < t) & (y_test == 1)) / sum(y_test == 1) for t in threshold_df['Threshold']]

threshold_df['Cost'] = (threshold_df['FP_Rate'] * sum(y_test == 0) * 100) + (threshold_df['FN_Rate'] * sum(y_test == 1) * 1000)

best_business_threshold = threshold_df.loc[threshold_df['Cost'].idxmin(), 'Threshold']
print(f"Best threshold for business impact: {best_business_threshold:.4f}")

# Save the results to a comprehensive report
threshold_df.to_csv('threshold_analysis.csv', index=False)
        """, language="python")

# Add footer with project information and group number
st.markdown("---")
st.markdown("AI and Decision Making Group Project | By Group 7") # The Chads ğŸ˜

# ğŸš¨ CHURN DETECTED. DEPLOYING MAXIMUM RIZZ ğŸš¨  
# If they leave, they leaveâ€¦ but not on **my** codeâ€™s watch ğŸ˜¤ğŸ”  
# This model stays **10 steps ahead**, predicting churn before they even THINK about it ğŸ§ ğŸ’¨  
# Banks finna **owe me their customer retention bonuses** ğŸ’°ğŸ‘€  
        # â€“ Wai Yan Moe (Chief Retention Officer ğŸ›¡ï¸, Data Rizzlord ğŸ’¸, Churn Assassin ğŸ¥·)